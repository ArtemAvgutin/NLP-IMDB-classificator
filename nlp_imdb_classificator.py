# -*- coding: utf-8 -*-
"""NLP-IMDB-classificator

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ItbD8aqdOBrn5__YMzgaIipdzpEbMpbS

## Задача бинарной классификации. Определение положительных и отрицательных отзывов на фильмы.

Отзывы делятся на:
- положительные (оценка >= 7) 1 - отзыв положительные
- отрицательные (оценка <= 4) 0 - отзыв отрицательные

Размер набора данных 50000 отзывов:
- Для обучения 25000
- Для тестирования 25000

Количество положительных и отрицательных отзывов одинаковое.

https://ai.stanford.edu/~amaas/data/sentiment/

https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf

#Импорт библиотек
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout, LSTM
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
# %matplotlib inline

"""## Загружаем данные"""

max_words=10000 #Используем самые популярные 10000 слов, можно больше

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)

"""## Просмотр данных

Рецензия
"""

x_train[11] #Токенизация слов

y_train[11] #Ответ рецензии 1 - положительный, 0 - отрицательный

"""В наборе данных IMDB используется частотное кодирование слов. Загрузим словарь, который использовался для кодирования. Теперь можно раскодировать слова."""

word_index = imdb.get_word_index()

word_index

# Преобразуем словарь, чтобы по номеру получать слово
reverse_word_index = dict()
for key, value in word_index.items():
    reverse_word_index[value] = key

# Печатаем 35 самых частых слов
for i in range(1, 36):
    print(i, '---->', reverse_word_index[i])

"""Теперь можно раскодировать весь отзыв по индексу

Служебные коды:
- 0 - символ заполнитель
- 1 - начало отзыва
- 2 - неизвестное слово
"""

index = 222
message = ''
for code in x_train[index]:
    word = reverse_word_index.get(code - 3, '!') # ! - Это замена символа для тех слов, которые не вошли в наш лимит, в моём случае, которые не вошли в 10000 выборки в самом начале
    message += word + ' '
message

"""## Подготовка данных для обучения"""

maxlen = 200 #Максимальная длинна слов в отзыве, если слов менее 200, мы подставляем нули. Так-как нам нужна одинковая длинна, мы подгоняем все отзывы к 200 словам

# В функции обрезаем, если более 200 и добавляем символ заполнитель (0), если менее 200
x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')
x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')

x_train[1]

y_train[1]

"""## Создание нейронной сети"""

model = Sequential()
model.add(Embedding(max_words, 2, input_length=maxlen))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

"""## Обучаем нейронную сеть"""

history = model.fit(x_train,
                    y_train,
                    epochs=15,
                    batch_size=128,
                    validation_split=0.1)

plt.plot(history.history['accuracy'],
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'],
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## Проверяем работу сети на тестовом наборе данных"""

scores = model.evaluate(x_test, y_test, verbose=1)

"""## Исследуем обученное плотное векторное представление слов

**Получаем матрицу плотных векторных представлений слов**
"""

embedding_matrix = model.layers[0].get_weights()[0]

embedding_matrix[:5]

"""**Загружаем словарь с номерами слов**"""

word_index_org = imdb.get_word_index()

"""Дополняем словарь служебными символами"""

word_index = dict()
for word,number in word_index_org.items():
    word_index[word] = number + 3
word_index["<Заполнитель>"] = 0
word_index["<Начало последовательности>"] = 1
word_index["<Неизвестное слово>"] = 2
word_index["<Не используется>"] = 3

"""**Ищем векторы для слов**"""

word = 'good'
word_number = word_index[word]
print('Номер слова', word_number)
print('Вектор для слова', embedding_matrix[word_number])

"""##Записываем плотные векторные представления в файл для более удобного просмотре, если нужно. Если не нужно, то просто пропускаем эти ячейки и переходим к графическим представлениям.

**Составляем реверсивный словарь токенов (слов)**
"""

reverse_word_index = dict()
for key, value in word_index.items():
    reverse_word_index[value] = key

filename = 'imdb_vectors_embeddings.csv'

with open(filename, 'w') as f:
    for word_num in range(max_words):
      word = reverse_word_index[word_num]
      vec = embedding_matrix[word_num]
      f.write(word + ",")
      f.write(','.join([str(x) for x in vec]) + "\n")

!head -n 20 $filename

"""**Сохраняем файл на локальный компьютер**"""

files.download('imdb_vectors_embeddings.csv')

"""## Визуализация плотных векторных представлений слов"""

plt.scatter(embedding_matrix[:,0], embedding_matrix[:,1])

"""Выбираем коды слов, по которым можно определить тональность отзыва"""

review = ['brilliant', 'fantastic', 'amazing', 'good',
          'bad', 'awful','crap', 'terrible', 'trash', 'worst']
enc_review = []
for word in review:
    enc_review.append(word_index[word])
enc_review

"""Получаем векторное представление интересующих нас слов"""

review_vectors = embedding_matrix[enc_review]
review_vectors

"""Визуализация обученного плотного векторного представления слов, по которым можно определить эмоциональную окраску текста"""

plt.scatter(review_vectors[:,0], review_vectors[:,1])
for i, txt in enumerate(review):
    plt.annotate(txt, (review_vectors[i,0], review_vectors[i,1]))

"""#Представление тональности текста в формате one hot encoding"""

x_train[3]

"""Так как у нас все отзывы предствлены в числовом формате, нам нужно преобразовать массив в вектор one hot encoding"""

def vectorize_sequences(sequences, dimension=10000): #Длинна вектора 10000
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1. #Там, где у нас будет слово, ставим 1, а где нет ничего - вектор заполняется нулями
    return results

x_train = vectorize_sequences(x_train, max_words)
x_test = vectorize_sequences(x_test, max_words)

x_train[0][:50]

"""В таком формате отзывов мы теряем позицию и кол-во раз встречаемых слов, способ работы похож на метод Bag of words (мешок слов)"""

len(x_train[0])

y_train[0]

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(max_words,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train,
                    y_train,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.1)

plt.plot(history.history['accuracy'],
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'],
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

scores = model.evaluate(x_test, y_test, verbose=1)

print("Доля верных ответов на тестовых данных:", round(scores[1] * 100), '%')

"""На графике видно, что сразу точность на высоком уровне, но потом падает из-за переобучения

# Определение тональности текста с помощью сети LSTM
"""

max_words=10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)
maxlen = 200
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
x_train[5002]

model = Sequential()
model.add(Embedding(max_words, 8, input_length=maxlen))
model.add(LSTM(32, recurrent_dropout = 0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train,
                    y_train,
                    epochs=15,
                    batch_size=128,
                    validation_split=0.1)

plt.plot(history.history['accuracy'],
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'],
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

scores = model.evaluate(x_test, y_test, verbose=1)

print("Доля верных ответов на тестовых данных:", round(scores[1] * 100), '%')

"""Как видно из графика, после 3 эпохи идёт переобучение, хотя точность неплохая"""